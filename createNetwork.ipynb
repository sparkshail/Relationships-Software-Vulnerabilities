{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "879969e2",
   "metadata": {
    "id": "879969e2"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4178bfc2",
   "metadata": {
    "id": "4178bfc2",
    "outputId": "b1632838-14f0-4f0d-b6a0-a51be591648a"
   },
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "import xml.etree.ElementTree as ET\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import gensim.similarities \n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import igraph as ig\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from scipy import spatial\n",
    "from scipy import sparse\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.csgraph import minimum_spanning_tree\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac20a0c4",
   "metadata": {
    "id": "ac20a0c4"
   },
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "998ab60f",
   "metadata": {
    "id": "998ab60f"
   },
   "outputs": [],
   "source": [
    "# reads in the CVE data set and returns vulnerability information  \n",
    "\n",
    "def read_file(file):\n",
    "    all_entries = {0: {'CVE': \"\", 'Date': \"\",'Description': [], 'Description-Tokenized' : [],\n",
    "                   'Month-String':\"\",'Month-Int': 0,\"Year\":\"\",\"Year-Int\":0,\"Cluster\": 0,\n",
    "                    \"Time Start\": 0, \"Time End\": 0,'Type':''}}\n",
    "    \n",
    "    untokenized_text = list()\n",
    "    start_index = 0\n",
    "    year_num = 1\n",
    "    \n",
    "    vulns = parse_data(file)\n",
    "    for text in vulns[1]:\n",
    "        untokenized_text.append(text)\n",
    "    all_entries = store_entries(vulns,start_index,year_num,len(file))\n",
    "        \n",
    "    return [all_entries,untokenized_text] \n",
    "        \n",
    "# helper method used to identify \"REJECT\" entries \n",
    "\n",
    "def is_member(target, possible):\n",
    "    for item in possible:\n",
    "        if target is item or target == item:\n",
    "              return True\n",
    "    return False\n",
    "\n",
    "# parse CVE dataset in a hierarchical format\n",
    "\n",
    "def parse_data(file):\n",
    "    tree = ET.parse(file)\n",
    "    root = tree.getroot()\n",
    "    notes = list()\n",
    "    cve = list()\n",
    "    pub = list()\n",
    "    for i in range(5,len(root)):\n",
    "        try:\n",
    "            #print(i)\n",
    "            token = word_tokenize(root[i][1][0].text)\n",
    "            if is_member(\"REJECT\",token) == False:  \n",
    "                pub.append(root[i][1][1].text)\n",
    "                cve.append(root[i][2].text)\n",
    "                notes.append(root[i][1][0].text)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    vulns_info = [cve,notes,pub]\n",
    "    return vulns_info\n",
    "            \n",
    "# methods for tokenizing     \n",
    "    \n",
    "def tokenize(text):\n",
    "    tokenized = list()\n",
    "    for i in range(0, len(text)):\n",
    "        filtered = remove_stopwords(word_tokenize(text[i]))\n",
    "        tokenized.append(filtered)\n",
    "                         \n",
    "    return tokenized\n",
    "\n",
    "def tokenize_single(sentence):\n",
    "    filtered = remove_stopwords(word_tokenize(sentence))\n",
    "    tokenized = [word.lower() for word in filtered] \n",
    "    return tokenized\n",
    "\n",
    "# method to remove stopwords and punctuation\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words.append(')')\n",
    "    stop_words.append('(')\n",
    "    stop_words.append('.')\n",
    "    stop_words.append('')\n",
    "    filtered_sentence = [w for w in text if not w.lower() in stop_words]\n",
    "    filtered_sentence = []\n",
    " \n",
    "    for w in text:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "    return filtered_sentence\n",
    "\n",
    "# read vulnerability text for doc2vec model training \n",
    "\n",
    "def read_corpus(text, tokens_only=False):\n",
    "    count=0\n",
    "    for doc in text:\n",
    "        count+=1\n",
    "        tokens = gensim.utils.simple_preprocess(doc)\n",
    "        if tokens_only:\n",
    "            yield tokens\n",
    "        else:\n",
    "            yield gensim.models.doc2vec.TaggedDocument(tokens, [count])\n",
    "\n",
    "            \n",
    "# build and train a doc2vec model on all vulnerability text          \n",
    "            \n",
    "def build_doc2vec(untokenized_text):\n",
    "    train_corpus = list(read_corpus(untokenized_text))\n",
    "    test_corpus = list(read_corpus(untokenized_text, tokens_only=True))\n",
    "    model = gensim.models.doc2vec.Doc2Vec(vector_size=100, min_count=2, epochs=20)\n",
    "    model.build_vocab(train_corpus)\n",
    "    model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    return model\n",
    "\n",
    "\n",
    "# converts the vulnerability text to vectors. text is a list of tokenized sentences for the entire data set\n",
    "# model is a doc2vec model \n",
    " \n",
    "def compute_vectors(text,model):\n",
    "    vectors = list()\n",
    "    for sentence in text:\n",
    "        vectors.append(model.infer_vector(sentence))\n",
    "    return vectors\n",
    "        \n",
    "    \n",
    "# store information of each vulnerability entry in a dictionary. Includes CVE ID, Publication date, vulnerability\n",
    "# description, tokenized description, month as a string, and month as an int (1-12)\n",
    "    \n",
    "def store_entries(vulns,start_index,year_num,total_years):\n",
    "    # Create dictionaries for every year \n",
    "    entries = {start_index: {'CVE': \"\", 'Date': \"\",'Description': [], 'Description-Tokenized' : [],\n",
    "                   'Month-String':\"\",'Month-Int': 0,\"Year\":\"\",\"Year-Int\":0,\"Time Start\": 0, \"Time End\": 0}\n",
    "                      'Type': ''}\n",
    "    j=0\n",
    "\n",
    "    for i in range(start_index,start_index+len(vulns[0])):\n",
    "        entries[i] = {}\n",
    "        entries[i]['CVE'] = vulns[0][j]\n",
    "        entries[i]['Date'] = vulns[2][j]\n",
    "        entries[i]['Description'] = vulns[1][j]\n",
    "        tokenized = tokenize_single(vulns[1][j])\n",
    "        desc_tokenized = remove_stopwords(tokenized)\n",
    "        entries[i]['Description-Tokenized'] = desc_tokenized\n",
    "        entries[i]['Month-String'] = \"n/a\"\n",
    "        entries[i]['Month-Int'] = 0\n",
    "        entries[i][\"Year\"] = entries[i]['Date'][0:4]\n",
    "        entries[i][\"Year-Int\"] = year_num\n",
    "        j = j + 1\n",
    "\n",
    "    for i in range(start_index,start_index+len(vulns[0])):\n",
    "        if str(entries[i]['Date'])[5] == '0' and entries[i]['Date'][6] == '1':\n",
    "            entries[i][\"Month-String\"] = \"January\"\n",
    "            entries[i][\"Month-Int\"] = 1\n",
    "        elif str(entries[i]['Date'])[5] == '0' and entries[i]['Date'][6] == '2':\n",
    "            entries[i][\"Month-String\"] = \"February\"\n",
    "            entries[i][\"Month-Int\"] = 2\n",
    "        elif str(entries[i]['Date'])[5] == '0' and entries[i]['Date'][6] == '3':\n",
    "            entries[i][\"Month-String\"] = \"March\"\n",
    "            entries[i][\"Month-Int\"] = 3\n",
    "        elif str(entries[i]['Date'])[5] == '0' and entries[i]['Date'][6] == '4':\n",
    "            entries[i][\"Month-String\"] = \"April\"\n",
    "            entries[i][\"Month-Int\"] = 4\n",
    "        elif str(entries[i]['Date'])[5] == '0' and entries[i]['Date'][6] == '5':\n",
    "            entries[i][\"Month-String\"] = \"May\"\n",
    "            entries[i][\"Month-Int\"] = 5\n",
    "        elif str(entries[i]['Date'])[5] == '0' and entries[i]['Date'][6] == '6':\n",
    "            entries[i][\"Month-String\"] = \"June\"\n",
    "            entries[i][\"Month-Int\"] = 6\n",
    "        elif str(entries[i]['Date'])[5] == '0' and entries[i]['Date'][6] == '7':\n",
    "            entries[i][\"Month-String\"] = \"July\"\n",
    "            entries[i][\"Month-Int\"] = 7\n",
    "        elif str(entries[i]['Date'])[5] == '0' and entries[i]['Date'][6] == '8':\n",
    "            entries[i][\"Month-String\"] = \"August\"\n",
    "            entries[i][\"Month-Int\"] = 8\n",
    "        elif str(entries[i]['Date'])[5] == '0' and entries[i]['Date'][6] == '9':\n",
    "            entries[i][\"Month-String\"] = \"September\"\n",
    "            entries[i][\"Month-Int\"] = 9\n",
    "        elif str(entries[i]['Date'])[5] == '1' and entries[i]['Date'][6] == '0':\n",
    "            entries[i][\"Month-String\"] = \"October\"\n",
    "            entries[i][\"Month-Int\"] = 10\n",
    "        elif str(entries[i]['Date'])[5] == '1' and entries[i]['Date'][6] == '1':\n",
    "            entries[i][\"Month-String\"] = \"November\"\n",
    "            entries[i][\"Month-Int\"] = 11\n",
    "        else:\n",
    "            entries[i][\"Month-String\"] = \"December\"\n",
    "            entries[i][\"Month-Int\"] = 12\n",
    "        \n",
    "        # get time interval \n",
    "        entries[i][\"Time Start\"] = (year_num * 12) - entries[i][\"Month-Int\"]\n",
    "        entries[i][\"Time End\"] = total_years * 12\n",
    "        \n",
    "        # get vulnerability type\n",
    "        entries[i]['Type'] = \n",
    "        \n",
    "    return entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49bb24d",
   "metadata": {},
   "source": [
    "# Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5d69d5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# creates an inverse cosine matrix \n",
    "\n",
    "def create_cos_matrix(vectors):\n",
    "    length = len(vectors)\n",
    "    matrix = np.zeros((length, length))\n",
    "    row_means =list() \n",
    "    for i in range(0,length):\n",
    "        for j in range(i,length):\n",
    "            mean = 0\n",
    "            cos = abs(1/(spatial.distance.cosine(vectors[i], vectors[j])-1))\n",
    "            matrix[i][j] = cos\n",
    "            matrix[j][i] = cos \n",
    "            mean +=cos\n",
    "        \n",
    "            if(cos == 1):\n",
    "                matrix[i][j] = 999\n",
    "                matrix[j][i] = 999\n",
    "        mean = mean/length\n",
    "        row_means.append(mean)\n",
    "        \n",
    "    for i in range(0,length):\n",
    "        for j in range(i,length):\n",
    "            if  matrix[i][j] < row_means[i]:\n",
    "                matrix[i][j] = 0\n",
    "                matrix[j][i] = 0 \n",
    "                \n",
    "    return matrix\n",
    "                \n",
    "# create a networkx graph from a cosine matrix \n",
    "    \n",
    "def create_network(cos_matrix):\n",
    "    X = sparse.csr_matrix(cos_matrix)\n",
    "    Tcsr = minimum_spanning_tree(X)\n",
    "    arr = Tcsr.toarray().astype(float)\n",
    "    G = nx.from_numpy_array(arr, parallel_edges=False, create_using=None)\n",
    "    return G\n",
    "\n",
    "    \n",
    "# convert networkx network to an igraph\n",
    "\n",
    "def create_igraph(G, entries,subnetwork = False):\n",
    "    # convert to igraph\n",
    "    h = ig.Graph.from_networkx(G)\n",
    "    weights_i = h.es[\"weight\"]\n",
    "    \n",
    "    if subnetwork == True:\n",
    "        return h\n",
    "    \n",
    "    spanning_tree = h.spanning_tree(weights=weights_i, return_tree=True)\n",
    "    start_times = list()\n",
    "    end_times = list()\n",
    "    for i in range(0,len(entries)):\n",
    "        start_times.append(entries[i]['Time Start'])\n",
    "        end_times.append(12)\n",
    "        \n",
    "    spanning_tree.vs[\"start_time\"] = start_times\n",
    "    spanning_tree.vs[\"end_time\"] = end_times\n",
    "    \n",
    "    edge_times = list()\n",
    "    for edge in spanning_tree.es:\n",
    "        target_vertex_id = edge.target\n",
    "        edge_times.append(entries[target_vertex_id]['Time Start'])\n",
    "    \n",
    "    spanning_tree.es[\"start_time\"] = edge_times\n",
    "    spanning_tree.es[\"end_time\"] = end_times\n",
    "\n",
    "    return spanning_tree\n",
    "    \n",
    "    \n",
    "# write edge attributes to a csv file \n",
    "\n",
    "def edge_list_to_csv(I,filename):\n",
    "    row_list = [[\"start_time\",\"end_time\",\"source\",\"target\",'weight']]\n",
    "    \n",
    "    for edge in I.es:\n",
    "        source = edge.source\n",
    "        target = edge.target\n",
    "        cur = [edge['start_time'],edge['end_time'],source,target,edge['weight']]\n",
    "        row_list.append(cur)\n",
    "        \n",
    "    with open(filename, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(row_list)\n",
    "        \n",
    "# write edge attributes to a csv file \n",
    "\n",
    "def node_list_to_csv(I,filename):\n",
    "    row_list = [[\"id\",\"start_time\",\"end_time\"]]\n",
    "    for node in I.vs:\n",
    "        row_list.append([node['_nx_name'],node['start_time'],node['end_time']])\n",
    "        \n",
    "    with open(filename, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(row_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a3f5c6",
   "metadata": {},
   "source": [
    "# Main Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5838a42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the name of the dataset file you would like to analyze: allitems-cvrf-year-1999.xml\n"
     ]
    }
   ],
   "source": [
    "# Ask user for filename of the node and edge attributes to be analyzed \n",
    "\n",
    "dataset_filename = str(input(\"Enter the name of the dataset file you would like to analyze: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "05ae561f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(file):\n",
    "    file_info = read_file(file)\n",
    "    entries = file_info[0]\n",
    "    text = file_info[1]\n",
    "    \n",
    "    # build a doc2vec model\n",
    "    read_corpus(text,tokens_only=False)\n",
    "    model = build_doc2vec(text)\n",
    "    tokenized = tokenize(text)\n",
    "    \n",
    "    vectors = compute_vectors(tokenized,model)\n",
    "    cos_matrix = create_cos_matrix(vectors)\n",
    "    G = create_network(cos_matrix)\n",
    "    I = create_igraph(G, entries,subnetwork = False)\n",
    "    \n",
    "    return I\n",
    "\n",
    "I = main(dataset_filename)\n",
    "\n",
    "edge_list_to_csv(I,'edge_list.csv')\n",
    "node_list_to_csv(I,'node_list.csv')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
